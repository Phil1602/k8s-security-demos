kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: egress-allow-cluster-internal-only
  namespace: monitoring
spec:
  policyTypes:
    - Egress
  podSelector: {}
  egress:
    - to:
        - namespaceSelector: {}
---
# Allowing IPs 10.0.0.0/8 might not include API server, even when the "kubernetes" service is within this range.
# It depends on the actual IP address that is resolved behind the service.
# On GKE this is an external IP.
# So find out the IP as follows and replace "APISERVER" by it: 
# kubectl get endpoints --namespace default kubernetes
# See https://stackoverflow.com/a/56494510/1845976
# Using "0.0.0.0/0" would be a workaround. It allows all egress. Works everywhere but does not adhere least privilege principle. 
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: egress-allow-prometheus-server-access-api-server
  namespace: monitoring
spec:
  policyTypes:
    - Egress
  podSelector:
    matchLabels:
      app: prometheus
      component: server
  egress:
    - to:
        - ipBlock:
            cidr: APISERVER
---
# Note: The following rules can also be generated by helm chart but seem to cause Problems when there are no Netpols for prometheus server.
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: ingress-allow-prometheus-to-access-kube-state-metrics
  namespace: monitoring
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: kube-state-metrics
  ingress:
    - ports:
        # No name in ksm pod
        - port: 8080
      from:
        - namespaceSelector:
            matchLabels:
              namespace: monitoring
          podSelector:
            matchLabels:
              app: prometheus
              component: server
---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: ingress-allow-prometheus-to-access-node-exporter
  namespace: monitoring
spec:
  podSelector:
    matchLabels:
      app: prometheus
      component: node-exporter
  ingress:
    - ports:
        - port: metrics
      from:
        - namespaceSelector:
            matchLabels:
              namespace: monitoring
          podSelector:
            matchLabels:
              app: prometheus
              component: server
---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: egress-allow-cluster-internal-only
  namespace: kube-system
spec:
  policyTypes:
    - Egress
  podSelector: {}
  egress:
    - to:
        - namespaceSelector: {}
---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: egress-allow-traefik-access-api-server
  namespace: kube-system
spec:
  policyTypes:
    - Egress
  podSelector:
    matchLabels:
      app: traefik
  egress:
    - to:
        - ipBlock:
            cidr: APISERVER
---
# If you want DNS resolution outside of the cluster 
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: egress-allow-kube-dns-access-outside
  namespace: kube-system
spec:
  policyTypes:
    - Egress
  podSelector:
    matchLabels:
      k8s-app: kube-dns
  egress:
    - ports:
        # TODO This port restriction does not seem to be effective, as kube DNS can still talk to API Server on port 443.
        # If this should ever work, Kube DNS must be allowed to talk to api server, just like traefik and prometheus.
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
    - to:
        - ipBlock:
            cidr: 0.0.0.0/0